
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [简介](#简介)
- [xv6-book](#xv6-book)
  - [chapter1](#chapter1)
  - [chapter2](#chapter2)
  - [chapter3](#chapter3)
- [lec](#lec)
  - [lec 1](#lec-1)
  - [lec 3](#lec-3)
  - [lec4](#lec4)
- [lab](#lab)
  - [lab guidance](#lab-guidance)
  - [Lab: Xv6 and Unix utilities](#lab-xv6-and-unix-utilities)
    - [prime](#prime)

<!-- /code_chunk_output -->


# 简介
----

- 环境：ubuntu20.04、qemu（模拟RISC-V64）、gdb-multiarch。
- 在RISC-V中，寄存器x0永远是0。
- 在xv6中，所有的用户都是root。
- 编译器可能会优化机器指令，所以要结合kernel.asm和函数对应的*.s文件。


# xv6-book
----

## chapter1

- 当用户程序调用一个system call，硬件提高特权等级（privilege level），并运行内核中的一段预先安排的代码。

- 内核为每个进程维护一个PID标识符

- `exec`system call会用调用的可执行文件（存储在文件系统中）覆盖进程的内存，但是会保留file table。`exec`不会返回，会从elf header中的entry开始执行。

- 每个进程都有用来存放fd的私有空间file table，fd从0开始。每个fd都有对应的offset，每次调用`read`或`write`都会从上一次地方开始。新分配的fd从最小的数字开始。

- pipe有2个接口：read和write，当write end和所有引用write end的fd关闭时，pipe才会关闭，这时候read读完缓冲区所有内容后下一次read返回0。pipe在内核中实现，有缓冲区。
![pipe](./pic/pipe.png)

- `mknod`创建特殊文件（称为device设备），需要传入2个数字用于唯一标识一个kernel device。当打开一个device file，内核会把`read`和`write`替换为kernel device implementation。

- 当一个文件的`link`为0且没有fd应用它时，释放文件的inode和磁盘空间。

## chapter2

- os需要满足三个需求：multiplexing，isolation and interaction（pipe）。

- 对物理资源的抽象：进程、文件等。通过系统调用（ecall）从用户陷入内核空间。riscv的三种模式：machine mode，supervisor（kernel） mode and user mode。

- 宏内核（monolithic kernel）和微内核（micro kernel）。微内核中，在用户空间像进程一样运行的os服务（如file server）成为servers。

- 进程的虚拟内存空间layout：
![process's virtual address](pic/va.png)
heap堆在需要时通过malloc分配空间。在虚拟地址空间最顶处分配了一页trampoline（包含进出内核的代码）和一页trapframe（save/restore用户进程的状态）。

- 每个进程有两个栈：user stack和kernel stack。

- xv6中，一个进程包含一个线程和一个地址空间。

- os必须假设进程的user-level代码会尽最大的可能去破坏内核或其他进程。

## chapter3

- satp寄存器
CPU中的satp寄存器保存root page table的地址，页表存在内存中的某个位置。MMU根据satp查找页表，MMU不负责建立页表，而是os负责。如果需要的物理地址（PM）不在页表中，MMU引发page-fault exception，由os负责从磁盘中寻找需要的page并移动到物理内存中。

- page table entry
![pte.png](./pic/pte.png)
  在Sv39 RISC-V中，只使用了39bits用于地址映射（共512GB），高位25bits用于未来拓展。在39bit中，前27bit用于三级页表，后12bit为物理内存offset，用于索引一页`4096(2^12)bytes`中的偏移。每级页表有512项，索引下一级页表时，offset全为0。

  为了避免从物理内存移动PTEs的代价，RSIC-V CPU使用TLB(Translation Look-aside Buffer)用于缓存PTEs。

>如果只使用一级页表，需要2^27表项，需要占用很大的空间。

- kernel address map
![pagemap](./pic/pagemap.png)
  xv6为每个进程维护一张pagetable，同时为内核维护一张kernel pagetable（直接映射）。

  有2个地址没有直接映射：
  1. trampoline page：映射2次，1次高地址，1次直接映射。
  2. kernel stack page：每个进程都有自己的kstack，伴随一页guard page。guard page不会在物理地址中映射空间，栈溢出直接导致page-fault。
  >如果kstack采用直接映射方式，guard page对应的物理地址将很难使用。


# lec
----

## lec 1

- 用户空间和内核空间的接口：system calls系统调用，`Look and behave like function calls, but they aren’t`。

## lec 3

- 0x80000000处，entry.S初始化栈等，此时为m mode -> start.c，初始化并进入s mode -> main.c

- 初始化之后运行userinit()，即第一个进程，里面嵌入了`initcode.S`，作用是`exec /init`（因为这时候还没有文件系统，所以需要用这种方式嵌入image）。risc-v通过`ecall`指令从用户空间进入内核空间`kernel/syscall.c`

## lec4

- 在MMU之前有对VM的cache，之后有对PM的cache。切换页表之后会flush TLB。多核CPU每核都有satp和TLB。

- 物理内存地址layout由硬件决定，所以开机boot结束后，跳转到`0x8000 0000`也是由硬件决定（该地址需要人为写到boot中用于跳转）。

- 卸载IO devices映射位置的指令，实际上是写到对应设备芯片或controller里。

- 物理地址里可能有一部分（高地址空间）unused，取决于板子上的DRAM大小和xv6限制（128MB）。

# lab
----

## lab guidance

- 难度：Easy（小于1h），Moderate（1-2h），Hard（>2h，不需要很多代码，但是需要一些技巧）。

- print

- gdb调试：在一个终端开启gdb server`make qemu-gdb`，在另一个终端开启gdb client`gdb or riscv64-linux-gnu-gdb`（ubuntu使用`gdb-multiarch`），会根据xv6-riscv/.gdbinit自动配置）。
    >如果出现类似下面的警告：
    `warning: File "<your path>/xv6-riscv/.gdbinit" auto-loading has been declined by your 'auto-load safe-path' set to "$debugdir:$datadir/auto-load".
    To enable execution of this file add
        add-auto-load-safe-path <your path>/xv6-riscv/.gdbinit
    line to your configuration file "~/.gdbinit".`
    按照提示做，在~/.gdbinit文件里添加`add-auto-load-safe-path <your path>/xv6-riscv/.gdbinit`。

- 查看`kernel.asm`，同时编译器编译内核后，会给每个user program生成对应的.asm。

- 内核崩溃时，报错信息里有pc寄存器的值，然后查看`kernel.asm`在哪个函数出错，或者运行`addr2line -e kernel/kernel pc-value`。

- `ctrl-a c`进入qemu的monitor，可以查看虚拟机的信息。`info mem`查看页表（使用`cpu`指令选择`info mem`查看哪个核，`make qemu CPUS=1`模拟单核）。

## Lab: Xv6 and Unix utilities

- `ctrl + a x`退出qemu，xv6没有ps命令，但是可以输入`ctrl + p`，内核会输出每个进程的信息。

### prime

- 并发模型：
  1. 基于共享内存和锁的并发模型：在单核时代，基本使用这种方法。但是在多核和分布式情况下，可能并不适用。
  2. csp模型：通过通信来共享内存。

- 任务：使用pipes编写素数筛的并发版本。使用pipe和fork建立pipeline，父进程投喂2-35进入pipeline。对于每个素数，安排一个进程通过pipe从left neighbor进程读取，然后通过另一个pipe写入right neighbor进程。

- 提示：
  1. 关闭不需要的fd，否则在到达35之前会耗尽xv6的资源。
  2. 父进程投喂完后需要等待pipeline完成，
  3. 当write end of pipe关闭时，read读完pipe中所有数据后下一次read会返回0。
  4. 当需要时才在pipeline中创建子进程。

- xv6资源有限，不能一次创建11个pipe，所以在循环里关闭不需要的管道再创建新的管道
    ```c
    int p[11][2];
    int data_p,data_n;
    
    pipe(p[0]); pipe(p[1]);
    for( int i=2; i<36; i++ )// feed
        write(p[0][1], &i, 4);

    for( int i=0; i<11; i++ ){
        if( fork() == 0 ){ // parent
            close unuse pipe
            print prime and sieve
            close used pipe
        }
        else{ //parent
            close unuse pipe
            create new pipe for next child
        }
    }
    ```